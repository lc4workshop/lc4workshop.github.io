<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
Design by TEMPLATED
http://templated.co
Released for free under the Creative Commons Attribution License
Name       : GrassyGreen 
Description: A two-column, fixed-width design with dark color scheme.
Version    : 1.0
Released   : 20140310
-->
<!-- main site from ACM to be added -->


<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>WCRML2019</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<link href="http://fonts.googleapis.com/css?family=Raleway:400,200,500,600,700,800,300" rel="stylesheet" />
<link href="default.css" rel="stylesheet" type="text/css" media="all" />
<link href="fonts.css" rel="stylesheet" type="text/css" media="all" />
<!--[if IE 6]>
<link href="default_ie6.css" rel="stylesheet" type="text/css" />
<![endif]-->
</head>
<body>
<div id="wrapper">
	<div id="menu-wrapper">
		<div id="menu" class="container">
			<ul>
				<li class="current_page_item"><a href="https://crossmodallearning.github.io/">Home</a></li>
				<li><a href=http://www.icmr2019.org/>ICMR2019</a></li>
				<li><a href=task.html>CALL FOR PAPERS</a></li>
				<li><a href=submission.html>AUTHORS</a></li>
				<li><a href=resources.html>PROGRAM</a></li>
				<li><a href=dates.html>ORGANIZATION</a></li>
				<!-- <li><a href=task.html>Task</a></li>
				<li><a href=example.html>Example</a></li>
				<li><a href=evaluation.html>Evaluation</a></li>
				
				-->
			</ul>
		</div>
		<!-- end #menu --> 
	<div id="header-wrapper">
		<div id="header" class="container">
			<div id="logo">
				<h1><a href="#">Crossmodal Learning and Application</a></h1>
				<h3><a href="#">June 10, Ottawa, Canada</a></h3>
				<!--<p>SemEval 2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications</p> -->
			</div>
		</div>
	</div>
	</div>
	
	<div id="page" class="container">
		<div id="content">
			<div class="title">
				<h2>Topics of Interest</h2>
				<span class="byline"></span> 
			</div>
	

<p>To contribute to the understanding of cross-modal technologies, we invite original articles in relevant topics, which include but are not limited to 
 </p>
			<ul>
			<li>    Multi modal representation/feature learning </li>
			<li> 	Cross-modal retrieval </li>
			<li>	Data alignment across modalities, e.g., synchronising motion sensor with video </li>
			<li>	Data translation, e.g., visually indicated sound </li>
			<li>	Learning using side information, e.g., modality hallucination </li>
			<li>	Knowledge transfer across modalities, e.g., zero-shot/few-shot learning </li>
			<li>	Applications with cross-modal data inlcuding IoT (Internet of Things), operation and maintenance, surveillance, public transportation, logistics, health care, task-oriented dialog, human-robot interaction with vision and audio, user/product/job search and recommendation, social media retrieval and analysis etc. </li>
			</ul>
			
			
			
			<p> We encourage submissions of both long and short papers.</p>
			<p> Accepted long andshort papers will be designated as oral presentations and posters, respectively</p>
			

		</div>
	</div>
	</div>
	
	</div>
</body>
</html>
